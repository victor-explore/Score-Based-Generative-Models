{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'im_channels' : 3,\n",
    "                'im_size' : 128,\n",
    "                'down_channels' : [32, 64, 128, 256, 256],\n",
    "                'mid_channels' : [256, 256, 256],\n",
    "                'down_sample' : [True, True, True, False],\n",
    "                'time_emb_dim' : 256,\n",
    "                'num_down_layers' : 1,\n",
    "                'num_mid_layers' : 1,\n",
    "                'num_up_layers' : 1,\n",
    "                'num_heads' : 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model architecture adapted from https://github.com/explainingai-code/DDPM-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 down_sample=True,  num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block\n",
    "    2. Attention block\n",
    "    3. Resnet block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels,  num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(8, out_channels)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    2. Concatenate Down block output\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels,  up_sample=True, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up_sample = up_sample\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, out_down):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config['im_channels']\n",
    "        self.down_channels = model_config['down_channels']\n",
    "        self.mid_channels = model_config['mid_channels']\n",
    "        self.down_sample = model_config['down_sample']\n",
    "        self.num_down_layers = model_config['num_down_layers']\n",
    "        self.num_mid_layers = model_config['num_mid_layers']\n",
    "        self.num_up_layers = model_config['num_up_layers']\n",
    "\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "\n",
    "        #Downsampling part\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1],\n",
    "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
    "        #Bottleneck part\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1],\n",
    "                                      num_layers=self.num_mid_layers))\n",
    "        #upsampling part\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
    "                                     up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1) # final convolution to match the number of channels =3\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.downs = [32, 64, 128, 256, 256]\n",
    "        # self.mids = [256, 256, 256]\n",
    "        # Image = B x 3 x 128 x 128\n",
    "        out = self.conv_in(x)   #[B x 3 x 128 x 128] --> [B x 32 x 128 x 128]\n",
    "\n",
    "        down_outs = []\n",
    "\n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out)\n",
    "        #[B x 32 x 128 x 128] --> [B x 64 x 64 x 64] --> [B x 128 x 32 x 32] --> [B x 256 x 16 x 16] --> [B x 256 x 16 x 16]\n",
    "\n",
    "\n",
    "        for mid in self.mids:\n",
    "            out = mid(out)\n",
    "\n",
    "        # out =  [B x 256 x 16 x 16]\n",
    "\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out)\n",
    "\n",
    "        #[B x 256 x 16 x 16] --> [B x 128 x 16 x 16] --> [B x 64 x 32 x 32] --> [B x 32 x 64 x 64] --> [B x 16 x 128 x 128]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)   #[B x 16 x 128 x 128] --> [B x 3 x 128 x 128]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCSN = Unet(model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"butterfly_dataset\":\n",
    "  sigmas = [150*(0.9)**i for i in range(90)]\n",
    "  sigmas = torch.tensor(sigmas).to(device)\n",
    "else:\n",
    "  sigmas = [450*(0.95)**i for i in range(200)]\n",
    "  sigmas = torch.tensor(sigmas).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, sigmas=sigmas):\n",
    "  \"\"\"The loss function for training Noise conditioned score networks.\n",
    "  Args:\n",
    "    model: An instance of Score model.\n",
    "    x: A mini-batch of training data.\n",
    "    sigmas: a tensor of shape [Number of noise levels,1] containing all the noise levels.\n",
    "\n",
    "  \"\"\"\n",
    "  # sampling uniformly a batch of noise levels.\n",
    "  random_indices = torch.randint(0,len(sigmas),(x.shape[0],)).to(device)\n",
    "  random_sigmas = sigmas[random_indices].unsqueeze(dim=-1).unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "\n",
    "  z = torch.randn_like(x)\n",
    "\n",
    "  perturbed_x = x + z * random_sigmas\n",
    "  score = model(perturbed_x)/random_sigmas #normalizing the score by dividing by noise level.\n",
    "  loss = torch.mean(torch.sum((score * random_sigmas + z)**2, dim=(1,2,3)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "dataloader = torch.utils.data.DataLoader(img_dataset, batch_size = 64, shuffle=True, num_workers  =4)\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "    for X,_ in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss  = loss_fn(model, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "    avg_loss = loss/len(dataloader)\n",
    "    print(f'epoch {epoch}: loss:{avg_loss}')\n",
    "save_dir = 'q6_ncsn'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(save_dir, 'model-weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## annealed lageavin dynamics for sampling\n",
    "def ALD(model, sigmas=sigmas, num_samples=100, num_timesteps=100,eps=1e-6, denoise=False):\n",
    "  \"\"\"The sampling algorithm for  generating new images.\n",
    "  Args:\n",
    "    model: A trained  Score model.\n",
    "    sigmas: a tensor of shape [Number of noise levels,1] containing all the noise levels.\n",
    "    num_timesteps: number of time steps to take at each noise level.\n",
    "    denoise: whether to apply the last setp denoising\n",
    "  \"\"\"\n",
    "    save_dir = 'generated_images'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xt = torch.randn(num_samples,3,128,128).to(device)\n",
    "        alphas = [eps*((sigmas[i])**2)/((sigmas[-1])**2) for i in range(len(sigmas))]\n",
    "        for i in tqdm(range(len(sigmas))):\n",
    "            for t in range(num_timesteps):\n",
    "                zt = torch.randn_like(xt).to(device)\n",
    "                xt = xt + alphas[i]*model(xt)/sigmas[i] + torch.sqrt(2*alphas[i])*zt\n",
    "            image = torch.clamp(xt,-1,1)    # bring the image to [-1,1] range.\n",
    "            image = (image+1)/2\n",
    "            image = torchvision.utils.make_grid(image, nrow=10)\n",
    "            image = image.permute(1,2,0).detach().cpu().numpy()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            plt.savefig(os.path.join(save_dir, f'sigma_step{sigmas[i]}.png'))\n",
    "            plt.close()\n",
    "    if denoise  == True\n",
    "      return xt + model(xt)*sigmas[-1]\n",
    "    else:\n",
    "      return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display final generated image\n",
    "image = ALD(model=NCSN)\n",
    "image = torch.clamp(image,-1,1)\n",
    "image = (image+1)/2\n",
    "image = torchvision.utils.make_grid(image, nrow=10)\n",
    "image = image.permute(1,2,0).detach().cpu().numpy()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fid calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3\n",
    "preprocess = transforms.Compose([       # Define preprocessing steps for Inception v3 input\n",
    "    transforms.Resize((299, 299)),  # Resize images to Inception v3 input size\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats ie we will also preprocess the data in same way as the original imagenet dataset that was used to train Inception v3\n",
    "])\n",
    "\n",
    "def prepare_inception_input(images):\n",
    "\n",
    "    \"\"\"Apply preprocessing to the input images\"\"\"\n",
    "    return preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inception_model():\n",
    "    \"\"\"Load inception pretrained v3 model for extracting features\"\"\"\n",
    "    model = inception_v3(pretrained=True, transform_input=False)  # Load pre-trained Inception v3 model without input transformation\n",
    "    model.fc = torch.nn.Identity()  # Removes the last fully connected layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model.to(device)  # Move the model to the same device as our GAN\n",
    "\n",
    "inception_model = load_inception_model()  # Load and prepare the modified Inception v3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    \"\"\"returns features of size [2048] extracted from the inception model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        features = inception_model(images)\n",
    "    return features  # This will be a tensor of shape [batch_size, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_real_features(dataloader, num_images=1000):\n",
    "    \"\"\"return features of real images extracted from the inception model\"\"\"\n",
    "    real_features = []  # List to store features of real images\n",
    "    image_count = 0  # Counter for processed images\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch in dataloader:\n",
    "            images = batch[0].to(device)  # Move batch of images to the device (assuming batch[0] contains images)\n",
    "            batch_size = images.size(0)  # Get the current batch size\n",
    "            if image_count + batch_size > num_images:\n",
    "                # If adding this batch would exceed num_images, only take what's needed\n",
    "                images = images[:num_images - image_count]\n",
    "            preprocessed_images = prepare_inception_input(images)  # Preprocess images for Inception v3\n",
    "            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n",
    "            real_features.append(batch_features)  # Add batch features to the list\n",
    "            image_count += batch_features.size(0)  # Update the count of processed images\n",
    "            if image_count >= num_images:\n",
    "                break  # Stop if we've processed enough images\n",
    "\n",
    "    real_features_tensor = torch.cat(real_features, dim=0)  # Concatenate all features into a single tensor\n",
    "    return real_features_tensor[:num_images]  # Return exactly num_images features\n",
    "\n",
    "# Extract features from 1000 real images\n",
    "real_features = extract_real_features(dataloader, num_images=1000)  # Extract features from 1000 real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_extract_features(sampler , num_samples=1000, batch_size=100):\n",
    "    \"\"\"return features of fake images extracted from the inception model\"\"\"\n",
    "    generated_features = []  # List to store features of generated images\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Calculate number of batches needed\n",
    "    # Set generator to evaluation mode\n",
    "    print(f\"Starting feature extraction for {num_samples} samples with batch size {batch_size}\")  # Print start of extraction process\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for batch_idx in range(num_batches):\n",
    "            current_batch_size = min(batch_size, num_samples - len(generated_features))  # Adjust batch size for last batch if needed\n",
    "            print(f\"Processing batch {batch_idx + 1}/{num_batches} with size {current_batch_size}\")  # Print current batch information\n",
    "            fake_images = sampler # Generate fake images\n",
    "            preprocessed_images = prepare_inception_input(fake_images)  # Preprocess images for Inception v3\n",
    "            batch_features = extract_features(preprocessed_images)  # Extract features using Inception v3\n",
    "            generated_features.append(batch_features)  # Add batch features to the list\n",
    "            print(f\"Extracted features shape: {batch_features.shape}\")  # Print shape of extracted features\n",
    "            if len(generated_features) * batch_size >= num_samples:\n",
    "                print(f\"Reached target number of samples. Stopping extraction.\")  # Print when target samples reached\n",
    "                break  # Stop if we've generated enough samples\n",
    "    print(f\"Feature extraction completed. Total features extracted: {len(generated_features) * batch_size}\")  # Print completion of extraction process\n",
    "\n",
    "    generated_features_tensor = torch.cat(generated_features, dim=0)  # Concatenate all features into a single tensor\n",
    "    return generated_features_tensor[:num_samples]  # Return exactly num_samples features\n",
    "\n",
    "# Generate 1000 samples and extract their features\n",
    "generated_features = generate_and_extract_features(sampler = ALD(model=NCSN))  # Generate and extract features from 1000 fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and covariance of real features\n",
    "real_mean = torch.mean(real_features, dim=0)  # Calculate mean across all samples for each feature\n",
    "real_cov = torch.cov(real_features.T)  # Calculate covariance matrix of features\n",
    "# Calculate mean and covariance of generated features\n",
    "generated_mean = torch.mean(generated_features, dim=0)  # Calculate mean across all samples for each feature\n",
    "generated_cov = torch.cov(generated_features.T)  # Calculate covariance matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance (FID) between real and generated image features.\n",
    "    Args:\n",
    "    real_mean (torch.Tensor): Mean of real image features.\n",
    "    real_cov (torch.Tensor): Covariance matrix of real image features.\n",
    "    generated_mean (torch.Tensor): Mean of generated image features.\n",
    "    generated_cov (torch.Tensor): Covariance matrix of generated image features.\n",
    "    Returns:\n",
    "    float: The calculated FID score.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert to numpy for scipy operations\n",
    "    real_mean_np = real_mean.cpu().numpy()  # Convert real mean to numpy array\n",
    "    real_cov_np = real_cov.cpu().numpy()  # Convert real covariance to numpy array\n",
    "    generated_mean_np = generated_mean.cpu().numpy()  # Convert generated mean to numpy array\n",
    "    generated_cov_np = generated_cov.cpu().numpy()  # Convert generated covariance to numpy array\n",
    "\n",
    "    # Calculate squared L2 norm between means\n",
    "    mean_diff = np.sum((real_mean_np - generated_mean_np) ** 2)  # Compute squared difference between means\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = scipy.linalg.sqrtm(real_cov_np.dot(generated_cov_np))  # Compute matrix square root\n",
    "    # Check and correct imaginary parts if necessary\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real  # Take only the real part if result is complex\n",
    "\n",
    "    # Calculate trace term\n",
    "    trace_term = np.trace(real_cov_np + generated_cov_np - 2 * covmean)  # Compute trace of the difference\n",
    "\n",
    "\n",
    "\n",
    "    # Compute FID\n",
    "    fid = mean_diff + trace_term  # Sum up mean difference and trace term\n",
    "    return fid  # Return FID as a Python float\n",
    "\n",
    "\n",
    "# Calculate FID for animal dataset using the above function\n",
    "fid_score = calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov)  # Compute FID score\n",
    "print(f\"Fréchet Inception Distance (FID): {fid_score:.4f}\")  # Print the calculated FID score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **NCSN trained on latent space of a trained vqvae**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters\n",
    "dimension_of_codebook_vectors = 128 # Dimension of the codebook vectors ie D\n",
    "number_of_codebook_vectors = 1024 # Number of codebook vectors ie K\n",
    "Commitment_cost = 1\n",
    "num_epochs = 50  # Number of training epochs, determines how long the model will train\n",
    "batch_size = 64  # Number of samples per gradient update, affects training speed and stability\n",
    "learning_rate = 0.001  # Learning rate for the optimizer, controls the step size during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1),  # Input: N x 3 x 128 x 128, Output: N x 64 x 64 x 64, 64 filters of size 4x4x3\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),  # Input: N x 64 x 64 x 64, Output: N x 128 x 32 x 32, 128 filters of size 4x4x64\n",
    "            nn.BatchNorm2d(128),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),  # Input: N x 128 x 32 x 32, Output: N x 256 x 16 x 16, 256 filters of size 4x4x128\n",
    "            nn.BatchNorm2d(256),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),  # Input: N x 256 x 16 x 16, Output: N x 512 x 8 x 8, 512 filters of size 4x4x256\n",
    "            nn.BatchNorm2d(512),  # Applies Batch Normalization to the output of the previous layer\n",
    "            nn.LeakyReLU(0.2),  # Applies Leaky ReLU activation function with negative slope of 0.2\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Conv2d(in_channels=512, out_channels=latent_dim, kernel_size=3, stride=1, padding=1)  # Input: N x 512 x 8 x 8, Output: N x latent_dim x 8 x 8, latent_dim filters of size 3x3x512\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # Apply main encoder layers\n",
    "        latents = self.final_conv(encoded)  # Generate latent vectors\n",
    "        return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim  # Dimension of each embedding vector\n",
    "        self.num_embeddings = num_embeddings  # Number of embedding vectors in the codebook\n",
    "        self.commitment_cost = commitment_cost  # Coefficient for the commitment loss\n",
    "\n",
    "        # Initialize the embedding vectors (codebook)\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)  # Creates an embedding layer to store the codebook\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)  # Initialize embedding weights uniformly\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Convert inputs from BCHW to BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()  # Rearrange dimensions from BCHW to BHWC\n",
    "\n",
    "        input_shape = inputs.shape  # Store original input shape\n",
    "\n",
    "        # Reshape inputs to (batch_size * height * width, channels)\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)  # Flatten input to 2D tensor\n",
    "\n",
    "        # Compute L2 distances between flattened input and embedding vectors\n",
    "        distances = torch.sum(flat_input**2, dim=1, keepdim=True) + \\\n",
    "                    torch.sum(self.embedding.weight**2, dim=1) - \\\n",
    "                    2 * torch.matmul(flat_input, self.embedding.weight.t())  # Calculate distances using the formula: ||x-y||^2 = ||x||^2 + ||y||^2 - 2x^T y\n",
    "\n",
    "        # Find nearest embedding for each input vector\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # Find index of nearest embedding for each input vector\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)  # Create one-hot encodings\n",
    "        encodings.scatter_(1, encoding_indices, 1)  # Set the corresponding index to 1 for each encoding\n",
    "\n",
    "        # Quantize the input vectors\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)  # Multiply encodings with embedding weights and reshape to original input shape\n",
    "\n",
    "        # Compute the VQ Losses\n",
    "        commitment_loss = F.mse_loss(quantized.detach(), inputs)  # Commitment loss: how far are the inputs from their quantized values\n",
    "        embedding_loss = F.mse_loss(quantized, inputs.detach())  # Embedding loss: how far are the quantized values from the inputs\n",
    "        vq_loss = embedding_loss + self.commitment_cost * commitment_loss  # Total VQ loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = inputs + (quantized - inputs).detach()  # Add quantization error to input (detached to avoid backpropagation through this path)\n",
    "\n",
    "        # Convert quantized from BHWC back to BCHW\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()  # Rearrange dimensions from BHWC back to BCHW\n",
    "\n",
    "        # Compute perplexity\n",
    "        avg_probs = torch.mean(encodings, dim=0)  # Average probability of each encoding across the batch\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))  # Compute perplexity (add small epsilon to avoid log(0))\n",
    "\n",
    "        return vq_loss, quantized, perplexity, encodings  # Return VQ loss, quantized vectors, perplexity, and encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim  # Store the latent dimension for use in the forward pass\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64, kernel_size=4, stride=2, padding=1),  # Input: N x latent_dim x 8 x 8, Output: N x 64 x 16 x 16, 64 filters of size 4x4xlatent_dim\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(64),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # Input: N x 64 x 16 x 16, Output: N x 32 x 32 x 32, 32 filters of size 4x4x64\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(32),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # Input: N x 32 x 32 x 32, Output: N x 16 x 64 x 64, 16 filters of size 4x4x32\n",
    "            nn.ReLU(),  # Apply ReLU activation to introduce non-linearity\n",
    "            nn.BatchNorm2d(16),  # Normalize the output to stabilize training\n",
    "\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),  # Input: N x 16 x 64 x 64, Output: N x 3 x 128 x 128, 3 filters of size 4x4x16\n",
    "            nn.Sigmoid()  # Apply Sigmoid activation to ensure output is in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        decoded_image = self.decoder(x)  # Pass the input through the decoder layers\n",
    "        return decoded_image  # Return the decoded RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings=512, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(embedding_dim)  # Initialize the encoder with the embedding dimension as latent dimension\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)  # Initialize the Vector Quantizer with default or provided values\n",
    "        self.decoder = Decoder(embedding_dim)  # Initialize the decoder with the embedding dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)  # Encode the input\n",
    "        vq_loss, quantized, perplexity, _ = self.vq_layer(z)  # Apply Vector Quantization\n",
    "        x_recon = self.decoder(quantized)  # Decode the quantized representation\n",
    "\n",
    "        return vq_loss, x_recon, perplexity  # Return VQ loss, reconstructed image, and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae =  VQVAE(embedding_dim=embedding_dim, num_embeddings=num_embeddings)   # Initializing the VQVAE model.\n",
    "vqvae.load_state_dict(torch.load('/kaggle/input/vqvaemodel/pytorch/default/1/vqvae_model_epoch_32.pth', weights_only=True))  # loading the trained weights of the vqvae model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'im_channels' : 128,\n",
    "                'im_size' : 8,\n",
    "                'down_channels' : [128, 128, 128, 256, 256],\n",
    "                'mid_channels' : [256, 256, 256],\n",
    "                'down_sample' : [True, True, False, False],\n",
    "                'time_emb_dim' : 256,\n",
    "                'num_down_layers' : 1,\n",
    "                'num_mid_layers' : 1,\n",
    "                'num_up_layers' : 1,\n",
    "                'num_heads' : 16}\n",
    "\n",
    "latent_unet = Unet(model_config).to(device)  #Initializing a Unet model for training on the latent space of vqvae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise levels for training ncsn on latent space of vqvae.\n",
    "sigmas = [100*(0.9)**i for i in range(80)]\n",
    "sigmas = torch.tensor(sigmas).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, sigmas=sigmas, eps=1e-5):\n",
    "  \"\"\"The loss function for training Noise conditioned score networks.\n",
    "  Args:\n",
    "    model: An instance of Score model.\n",
    "    x: A mini-batch of training data.\n",
    "    sigmas: a tensor of shape [Number of noise levels,1] containing all the noise levels.\n",
    "  \"\"\"\n",
    "  random_indices = torch.randint(0,len(sigmas),(x.shape[0],)).to(device)\n",
    "  random_sigmas = sigmas[random_indices].unsqueeze(dim=-1).unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "  z = torch.randn_like(x)\n",
    "\n",
    "  perturbed_x = x + z * random_sigmas\n",
    "  score = model(perturbed_x)/random_sigmas\n",
    "  loss = torch.mean(torch.sum((score * random_sigmas + z)**2, dim=(1,2,3)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = latent_unet.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "dataloader = torch.utils.data.DataLoader(butterfly_dataset, batch_size = 64, shuffle=True, num_workers  =4)\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "    for X,_ in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            X = vqvae.encoder(X)\n",
    "            _,X, _, _ = self.vq_layer(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss  = loss_fn(model, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "    avg_loss = loss/len(dataloader)\n",
    "    print(f'epoch {epoch}: loss:{avg_loss}')\n",
    "save_dir = 'vqvae_ncsn'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(save_dir, 'model-weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALD_vqvae(model, sigmas=sigmas, num_samples=100, num_timesteps=100,eps=1e-4):\n",
    "  \"\"\"The sampling algorithm for  generating new images from latent space of vqvae.\n",
    "  Args:\n",
    "    model: A trained  Score model.\n",
    "    sigmas: a tensor of shape [Number of noise levels,1] containing all the noise levels.\n",
    "    num_timesteps: number of time steps to take at each noise level.\n",
    "    denoise: whether to apply the last setp denoising\n",
    "  \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xt = torch.randn(num_samples,128,8,8).to(device)\n",
    "\n",
    "        alphas = [eps*((sigmas[i])**2)/((sigmas[-1])**2) for i in range(len(sigmas))]\n",
    "        for i in tqdm(range(len(sigmas))):\n",
    "            for t in range(num_timesteps):\n",
    "                zt = torch.randn_like(xt).to(device)\n",
    "                xt = xt + alphas[i]*model(xt)/sigmas[i] + torch.sqrt(2*alphas[i])*zt\n",
    "\n",
    "        image = xt + model(xt)*sigmas[-1]\n",
    "        image = vqvae.decoder(image)\n",
    "        return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot generated images\n",
    "image = ALD_vqvae(model)\n",
    "image = (image+1)/2\n",
    "image = torchvision.utils.make_grid(image, nrow=10)\n",
    "image = image.permute(1,2,0).detach().cpu().numpy()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
